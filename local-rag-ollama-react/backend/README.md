# ğŸ§  Local RAG with Ollama, LangChain & React

Sistema **RAG (Retrieval-Augmented Generation)** local que permite subir documentos (PDF, DOCX, TXT), indexarlos en una base vectorial **Chroma**, y realizar consultas semÃ¡nticas a travÃ©s de un modelo de lenguaje (**LLM**) servido por **Ollama** o **Azure OpenAI**.  
El frontend estÃ¡ desarrollado en **React + Vite + TailwindCSS**, y el backend en **FastAPI**.

---

## ğŸš€ CaracterÃ­sticas principales

- ğŸ” **Carga e indexado de documentos** (PDF, DOCX, TXT)  
- ğŸ§  **BÃºsqueda semÃ¡ntica** mediante embeddings locales de **Ollama** (`mxbai-embed-large`)
- ğŸ’¬ **Chat contextual** con los documentos indexados  
- ğŸ§© **Backend** en FastAPI + LangChain + Chroma  
- âš›ï¸ **Frontend** en React + Vite + TailwindCSS  
- ğŸ³ **Infraestructura Dockerizada** con `docker-compose`  
- ğŸŒ **Compatibilidad con Ollama o Azure OpenAI**  

---

## ğŸ—‚ï¸ Estructura del proyecto

```
local-rag-ollama-react/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ chroma_db/
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ extractor.py
â”‚   â”‚   â”œâ”€â”€ langgraph_nodes.py
â”‚   â”‚   â””â”€â”€ settings.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ .env
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ .env
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## ğŸ§© Modelos requeridos

El backend usa dos modelos de **Ollama**:

| PropÃ³sito | Modelo | Comando |
|----------|--------|---------|
| Embeddings | `mxbai-embed-large` | `ollama pull mxbai-embed-large` |
| LLM Chat | `llama3.1:8b` | `ollama pull llama3.1:8b` |

Si usÃ¡s Docker:
```bash
docker exec -it ollama ollama pull mxbai-embed-large
docker exec -it ollama ollama pull llama3.1:8b
```

---

## ğŸ³ EjecuciÃ³n con Docker Compose

### 1ï¸âƒ£ Crear los archivos `.env`

#### `backend/.env`
```
DATA_DIR=/app/data
OLLAMA_URL=http://ollama:11434
OLLAMA URL to see the downloaded models=http://localhost:11434/api/tags
EMBED_MODEL=mxbai-embed-large
LLM_MODEL=llama3.1:8b
```

#### `frontend/.env`
```
VITE_API_URL=http://localhost:8000
```

---

### 2ï¸âƒ£ Levantar los servicios

```bash
docker compose up --build
```

Servicios:
- ğŸ¦™ **Ollama** â†’ `localhost:11434`  
- âš™ï¸ **Backend FastAPI** â†’ `localhost:8000`  
- ğŸ’» **Frontend React** â†’ `localhost:5173`

---

## ğŸ§¾ Endpoints principales

| MÃ©todo | Endpoint | DescripciÃ³n |
|--------|-----------|-------------|
| `POST` | `/upload` | Sube un documento |
| `POST` | `/index/{doc_id}` | Indexa un documento |
| `GET`  | `/documents` | Lista documentos |
| `DELETE` | `/documents/{doc_id}` | Elimina un documento |
| `POST` | `/query` | Consulta con contexto |

---

## ğŸ§° Comandos Ãºtiles

```bash
docker compose logs -f backend
docker compose down -v
docker exec -it rag_backend bash
```

---

## ğŸ“„ Licencia
MIT License.
